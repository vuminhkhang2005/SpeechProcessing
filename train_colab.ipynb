{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéôÔ∏è Speech Denoising - Google Colab Training\n",
    "\n",
    "This notebook allows you to train the Speech Denoising U-Net model on Google Colab with GPU acceleration.\n",
    "\n",
    "## Overview\n",
    "- **Model**: U-Net with Complex Ratio Mask (CRM)\n",
    "- **Dataset**: VoiceBank + DEMAND\n",
    "- **Training Time**: ~1-2 hours on Colab GPU (T4/P100)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected! Go to Runtime > Change runtime type > GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository (uncomment if running from GitHub)\n",
    "# !git clone https://github.com/YOUR_USERNAME/speech_denoising.git\n",
    "# %cd speech_denoising\n",
    "\n",
    "# Or upload files manually and set the working directory\n",
    "import os\n",
    "# Uncomment the next line if you uploaded the project to a specific folder\n",
    "# os.chdir('/content/speech_denoising')\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch torchaudio --upgrade\n",
    "!pip install -q librosa soundfile scipy numpy pandas\n",
    "!pip install -q pystoi matplotlib seaborn tensorboard\n",
    "!pip install -q tqdm pyyaml\n",
    "\n",
    "# Optional: Install PESQ for additional metrics (may fail on some systems)\n",
    "!pip install -q pesq || echo \"PESQ installation failed - continuing without it\"\n",
    "\n",
    "print(\"\\n‚úÖ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Download Dataset\n",
    "\n",
    "You have two options:\n",
    "1. **Option A**: Download directly to Colab (faster, but lost when session ends)\n",
    "2. **Option B**: Mount Google Drive and store dataset there (persistent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Download directly to Colab\n",
    "# This is faster but data will be lost when the session ends\n",
    "\n",
    "DOWNLOAD_DIRECTLY = True  # Set to False if using Google Drive\n",
    "\n",
    "if DOWNLOAD_DIRECTLY:\n",
    "    !mkdir -p data\n",
    "    \n",
    "    # Download VoiceBank + DEMAND dataset from Edinburgh DataShare\n",
    "    # Note: These are large files (~3.3 GB total)\n",
    "    \n",
    "    print(\"üì• Downloading VoiceBank + DEMAND dataset...\")\n",
    "    print(\"This may take 10-20 minutes depending on connection speed.\\n\")\n",
    "    \n",
    "    # Dataset URLs from Edinburgh DataShare\n",
    "    BASE_URL = \"https://datashare.ed.ac.uk/bitstream/handle/10283/2791\"\n",
    "    \n",
    "    files = [\n",
    "        \"clean_trainset_28spk_wav.zip\",\n",
    "        \"noisy_trainset_28spk_wav.zip\", \n",
    "        \"clean_testset_wav.zip\",\n",
    "        \"noisy_testset_wav.zip\"\n",
    "    ]\n",
    "    \n",
    "    for f in files:\n",
    "        if not os.path.exists(f\"data/{f.replace('.zip', '')}\"):\n",
    "            print(f\"Downloading {f}...\")\n",
    "            !wget -q --show-progress -O data/{f} \"{BASE_URL}/{f}?sequence=1&isAllowed=y\"\n",
    "            print(f\"Extracting {f}...\")\n",
    "            !cd data && unzip -q {f} && rm {f}\n",
    "        else:\n",
    "            print(f\"‚úì {f.replace('.zip', '')} already exists\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Dataset downloaded and extracted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option B: Mount Google Drive (persistent storage)\n",
    "# Uncomment this cell if you want to use Google Drive\n",
    "\n",
    "USE_GOOGLE_DRIVE = False  # Set to True to use Google Drive\n",
    "GDRIVE_DATA_PATH = \"/content/drive/MyDrive/speech_denoising_data\"  # Change this path\n",
    "\n",
    "if USE_GOOGLE_DRIVE:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Create symbolic link to data directory\n",
    "    !mkdir -p \"{GDRIVE_DATA_PATH}\"\n",
    "    !ln -sf \"{GDRIVE_DATA_PATH}\" data\n",
    "    \n",
    "    print(f\"\\nüìÅ Data directory linked to: {GDRIVE_DATA_PATH}\")\n",
    "    print(\"If dataset is not there, download it using Option A first,\")\n",
    "    print(\"then copy the data folder to Google Drive for future sessions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(\"data\")\n",
    "required_dirs = [\n",
    "    \"clean_trainset_28spk_wav\",\n",
    "    \"noisy_trainset_28spk_wav\",\n",
    "    \"clean_testset_wav\",\n",
    "    \"noisy_testset_wav\"\n",
    "]\n",
    "\n",
    "print(\"üìä Dataset verification:\")\n",
    "all_ok = True\n",
    "for dir_name in required_dirs:\n",
    "    dir_path = data_dir / dir_name\n",
    "    if dir_path.exists():\n",
    "        count = len(list(dir_path.glob(\"*.wav\")))\n",
    "        print(f\"  ‚úÖ {dir_name}: {count} files\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå {dir_name}: NOT FOUND\")\n",
    "        all_ok = False\n",
    "\n",
    "if all_ok:\n",
    "    print(\"\\n‚úÖ Dataset is ready for training!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Some directories are missing. Please download the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Configuration\n",
    "\n",
    "Adjust training parameters here. The defaults are optimized for Colab GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration for Colab\n",
    "# These settings are optimized for Colab's GPU (T4/P100)\n",
    "\n",
    "CONFIG = {\n",
    "    # Data paths\n",
    "    'data': {\n",
    "        'train_clean_dir': './data/clean_trainset_28spk_wav',\n",
    "        'train_noisy_dir': './data/noisy_trainset_28spk_wav',\n",
    "        'test_clean_dir': './data/clean_testset_wav',\n",
    "        'test_noisy_dir': './data/noisy_testset_wav',\n",
    "        'sample_rate': 16000,\n",
    "        'segment_length': 32000,  # 2 seconds\n",
    "    },\n",
    "    \n",
    "    # STFT parameters\n",
    "    'stft': {\n",
    "        'n_fft': 512,\n",
    "        'hop_length': 128,\n",
    "        'win_length': 512,\n",
    "    },\n",
    "    \n",
    "    # Model parameters\n",
    "    'model': {\n",
    "        'name': 'UNetDenoiser',\n",
    "        'encoder_channels': [32, 64, 128, 256, 512],\n",
    "        'use_attention': True,\n",
    "        'dropout': 0.1,\n",
    "    },\n",
    "    \n",
    "    # Training parameters (optimized for Colab)\n",
    "    'training': {\n",
    "        'batch_size': 8,  # Reduced for Colab GPU memory\n",
    "        'num_epochs': 50,  # Reduced for faster training\n",
    "        'learning_rate': 0.0001,\n",
    "        'weight_decay': 1e-5,\n",
    "        'scheduler': {\n",
    "            'patience': 5,\n",
    "            'factor': 0.5,\n",
    "            'min_lr': 1e-6,\n",
    "        },\n",
    "        'early_stopping_patience': 10,\n",
    "        'grad_clip': 5.0,\n",
    "        'num_workers': 2,  # Reduced for Colab\n",
    "        'use_amp': True,  # Mixed precision for faster training\n",
    "    },\n",
    "    \n",
    "    # Loss\n",
    "    'loss': {\n",
    "        'l1_weight': 1.0,\n",
    "        'stft_weight': 1.0,\n",
    "    },\n",
    "    \n",
    "    # Checkpoints\n",
    "    'checkpoint': {\n",
    "        'save_dir': './checkpoints',\n",
    "        'save_every': 5,\n",
    "        'keep_last': 3,\n",
    "    },\n",
    "    \n",
    "    # Logging\n",
    "    'logging': {\n",
    "        'log_dir': './logs',\n",
    "        'log_every': 50,\n",
    "    },\n",
    "    \n",
    "    # Evaluation\n",
    "    'eval': {\n",
    "        'output_dir': './outputs',\n",
    "        'compute_pesq': True,\n",
    "        'compute_stoi': True,\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üìã Configuration loaded!\")\n",
    "print(f\"  Batch size: {CONFIG['training']['batch_size']}\")\n",
    "print(f\"  Epochs: {CONFIG['training']['num_epochs']}\")\n",
    "print(f\"  Learning rate: {CONFIG['training']['learning_rate']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Initialize Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "from data.dataset import VoiceBankDEMANDDataset, create_dataloaders\n",
    "from models.unet import UNetDenoiser\n",
    "from models.loss import DenoiserLoss\n",
    "from utils.metrics import evaluate_batch, is_pesq_available\n",
    "from utils.audio_utils import AudioProcessor\n",
    "\n",
    "print(\"‚úÖ Modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "data_cfg = CONFIG['data']\n",
    "stft_cfg = CONFIG['stft']\n",
    "train_cfg = CONFIG['training']\n",
    "\n",
    "print(\"üìÇ Loading dataset...\")\n",
    "train_loader, val_loader = create_dataloaders(\n",
    "    train_clean_dir=data_cfg['train_clean_dir'],\n",
    "    train_noisy_dir=data_cfg['train_noisy_dir'],\n",
    "    test_clean_dir=data_cfg['test_clean_dir'],\n",
    "    test_noisy_dir=data_cfg['test_noisy_dir'],\n",
    "    sample_rate=data_cfg['sample_rate'],\n",
    "    segment_length=data_cfg['segment_length'],\n",
    "    batch_size=train_cfg['batch_size'],\n",
    "    num_workers=train_cfg['num_workers'],\n",
    "    n_fft=stft_cfg['n_fft'],\n",
    "    hop_length=stft_cfg['hop_length'],\n",
    "    win_length=stft_cfg['win_length']\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Data loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model_cfg = CONFIG['model']\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = UNetDenoiser(\n",
    "    in_channels=2,\n",
    "    out_channels=2,\n",
    "    encoder_channels=model_cfg['encoder_channels'],\n",
    "    use_attention=model_cfg['use_attention'],\n",
    "    dropout=model_cfg['dropout'],\n",
    "    mask_type='CRM'\n",
    ").to(device)\n",
    "\n",
    "print(f\"üß† Model: {model_cfg['name']}\")\n",
    "print(f\"   Parameters: {model.count_parameters():,}\")\n",
    "print(f\"   Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Create directories\n",
    "ckpt_dir = Path(CONFIG['checkpoint']['save_dir'])\n",
    "ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Loss function\n",
    "loss_cfg = CONFIG['loss']\n",
    "criterion = DenoiserLoss(\n",
    "    complex_weight=loss_cfg['l1_weight'],\n",
    "    magnitude_weight=1.0,\n",
    "    stft_weight=loss_cfg['stft_weight'],\n",
    "    use_mr_stft=True,\n",
    "    n_fft=stft_cfg['n_fft'],\n",
    "    hop_length=stft_cfg['hop_length'],\n",
    "    win_length=stft_cfg['win_length']\n",
    ").to(device)\n",
    "\n",
    "# Audio processor\n",
    "audio_processor = AudioProcessor(\n",
    "    n_fft=stft_cfg['n_fft'],\n",
    "    hop_length=stft_cfg['hop_length'],\n",
    "    win_length=stft_cfg['win_length']\n",
    ")\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=train_cfg['learning_rate'],\n",
    "    weight_decay=train_cfg['weight_decay']\n",
    ")\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=train_cfg['scheduler']['factor'],\n",
    "    patience=train_cfg['scheduler']['patience'],\n",
    "    min_lr=train_cfg['scheduler']['min_lr']\n",
    ")\n",
    "\n",
    "# Mixed precision scaler\n",
    "scaler = GradScaler() if train_cfg['use_amp'] else None\n",
    "\n",
    "print(\"‚úÖ Training components initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_epoch(model, train_loader, optimizer, criterion, audio_processor, device, scaler=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=\"Training\")\n",
    "    for batch in pbar:\n",
    "        noisy_stft = batch['noisy_stft'].to(device)\n",
    "        clean_stft = batch['clean_stft'].to(device)\n",
    "        clean_wav = batch['clean'].to(device)\n",
    "        \n",
    "        # Reshape: [batch, freq, time, 2] -> [batch, 2, freq, time]\n",
    "        noisy_stft = noisy_stft.permute(0, 3, 1, 2)\n",
    "        clean_stft = clean_stft.permute(0, 3, 1, 2)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if scaler is not None:\n",
    "            with autocast():\n",
    "                pred_stft = model(noisy_stft)\n",
    "                \n",
    "                # Reconstruct waveform\n",
    "                pred_stft_istft = pred_stft.permute(0, 2, 3, 1)\n",
    "                pred_wav = audio_processor.istft(pred_stft_istft)\n",
    "                \n",
    "                min_len = min(pred_wav.shape[-1], clean_wav.shape[-1])\n",
    "                pred_wav = pred_wav[..., :min_len]\n",
    "                clean_wav_trim = clean_wav[..., :min_len]\n",
    "                \n",
    "                losses = criterion(pred_stft, clean_stft, pred_wav, clean_wav_trim)\n",
    "            \n",
    "            scaler.scale(losses['total_loss']).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), train_cfg['grad_clip'])\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            pred_stft = model(noisy_stft)\n",
    "            pred_stft_istft = pred_stft.permute(0, 2, 3, 1)\n",
    "            pred_wav = audio_processor.istft(pred_stft_istft)\n",
    "            \n",
    "            min_len = min(pred_wav.shape[-1], clean_wav.shape[-1])\n",
    "            pred_wav = pred_wav[..., :min_len]\n",
    "            clean_wav_trim = clean_wav[..., :min_len]\n",
    "            \n",
    "            losses = criterion(pred_stft, clean_stft, pred_wav, clean_wav_trim)\n",
    "            losses['total_loss'].backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), train_cfg['grad_clip'])\n",
    "            optimizer.step()\n",
    "        \n",
    "        total_loss += losses['total_loss'].item()\n",
    "        num_batches += 1\n",
    "        pbar.set_postfix({'loss': f\"{losses['total_loss'].item():.4f}\"})\n",
    "    \n",
    "    return total_loss / num_batches\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, val_loader, criterion, audio_processor, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    metrics = {'stoi': 0, 'si_sdr': 0}\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "        noisy_stft = batch['noisy_stft'].to(device)\n",
    "        clean_stft = batch['clean_stft'].to(device)\n",
    "        clean_wav = batch['clean'].to(device)\n",
    "        \n",
    "        noisy_stft = noisy_stft.permute(0, 3, 1, 2)\n",
    "        clean_stft = clean_stft.permute(0, 3, 1, 2)\n",
    "        \n",
    "        pred_stft = model(noisy_stft)\n",
    "        pred_stft_istft = pred_stft.permute(0, 2, 3, 1)\n",
    "        pred_wav = audio_processor.istft(pred_stft_istft)\n",
    "        \n",
    "        min_len = min(pred_wav.shape[-1], clean_wav.shape[-1])\n",
    "        pred_wav = pred_wav[..., :min_len]\n",
    "        clean_wav_trim = clean_wav[..., :min_len]\n",
    "        \n",
    "        losses = criterion(pred_stft, clean_stft)\n",
    "        total_loss += losses['total_loss'].item()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        try:\n",
    "            batch_metrics = evaluate_batch(\n",
    "                clean_wav_trim, pred_wav,\n",
    "                sample_rate=data_cfg['sample_rate'],\n",
    "                compute_pesq=False,\n",
    "                compute_stoi=True\n",
    "            )\n",
    "            for key in metrics:\n",
    "                if key in batch_metrics:\n",
    "                    metrics[key] += batch_metrics[key]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    avg_metrics = {k: v / num_batches for k, v in metrics.items()}\n",
    "    \n",
    "    return avg_loss, avg_metrics\n",
    "\n",
    "print(\"‚úÖ Training functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop\n",
    "print(\"=\"*60)\n",
    "print(\"üöÄ STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Epochs: {train_cfg['num_epochs']}\")\n",
    "print(f\"Batch size: {train_cfg['batch_size']}\")\n",
    "print(f\"Device: {device}\")\n",
    "print()\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "history = {'train_loss': [], 'val_loss': [], 'stoi': [], 'si_sdr': []}\n",
    "\n",
    "for epoch in range(train_cfg['num_epochs']):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{train_cfg['num_epochs']}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion, audio_processor, device, scaler)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_metrics = validate(model, val_loader, criterion, audio_processor, device)\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['stoi'].append(val_metrics.get('stoi', 0))\n",
    "    history['si_sdr'].append(val_metrics.get('si_sdr', 0))\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"  STOI: {val_metrics.get('stoi', 0):.3f}\")\n",
    "    print(f\"  SI-SDR: {val_metrics.get('si_sdr', 0):.2f} dB\")\n",
    "    print(f\"  LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "    \n",
    "    # Check for best model\n",
    "    is_best = val_loss < best_val_loss\n",
    "    if is_best:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        # Save best model\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'config': CONFIG\n",
    "        }, ckpt_dir / 'best_model.pt')\n",
    "        print(\"  ‚úÖ Saved best model!\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    # Save periodic checkpoint\n",
    "    if (epoch + 1) % CONFIG['checkpoint']['save_every'] == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "        }, ckpt_dir / f'checkpoint_epoch_{epoch+1}.pt')\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= train_cfg['early_stopping_patience']:\n",
    "        print(f\"\\n‚èπÔ∏è Early stopping at epoch {epoch + 1}\")\n",
    "        break\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ TRAINING COMPLETED!\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"Model saved to: {ckpt_dir / 'best_model.pt'}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Loss\n",
    "axes[0, 0].plot(history['train_loss'], label='Train')\n",
    "axes[0, 0].plot(history['val_loss'], label='Validation')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Training & Validation Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# STOI\n",
    "axes[0, 1].plot(history['stoi'])\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('STOI')\n",
    "axes[0, 1].set_title('STOI (Speech Intelligibility)')\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# SI-SDR\n",
    "axes[1, 0].plot(history['si_sdr'])\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('SI-SDR (dB)')\n",
    "axes[1, 0].set_title('SI-SDR (Signal Quality)')\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Hide empty subplot\n",
    "axes[1, 1].axis('off')\n",
    "axes[1, 1].text(0.5, 0.5, f'Best Val Loss: {best_val_loss:.4f}\\n\\n'\n",
    "                f'Final STOI: {history[\"stoi\"][-1]:.3f}\\n'\n",
    "                f'Final SI-SDR: {history[\"si_sdr\"][-1]:.2f} dB',\n",
    "                ha='center', va='center', fontsize=14,\n",
    "                transform=axes[1, 1].transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Training history saved to training_history.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model and test on a sample\n",
    "checkpoint = torch.load(ckpt_dir / 'best_model.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"‚úÖ Loaded best model from epoch {checkpoint['epoch'] + 1}\")\n",
    "print(f\"   Validation loss: {checkpoint['val_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a sample from validation set\n",
    "import torchaudio\n",
    "from utils.audio_utils import load_audio, save_audio\n",
    "import IPython.display as ipd\n",
    "\n",
    "# Get a test sample\n",
    "test_batch = next(iter(val_loader))\n",
    "noisy_wav = test_batch['noisy'][0:1].to(device)\n",
    "clean_wav = test_batch['clean'][0:1]\n",
    "noisy_stft = test_batch['noisy_stft'][0:1].to(device)\n",
    "\n",
    "# Denoise\n",
    "with torch.no_grad():\n",
    "    noisy_stft_input = noisy_stft.permute(0, 3, 1, 2)\n",
    "    pred_stft = model(noisy_stft_input)\n",
    "    pred_stft_out = pred_stft.permute(0, 2, 3, 1)\n",
    "    denoised_wav = audio_processor.istft(pred_stft_out)\n",
    "\n",
    "# Convert to numpy for playback\n",
    "noisy_np = noisy_wav[0].cpu().numpy()\n",
    "clean_np = clean_wav[0].numpy()\n",
    "denoised_np = denoised_wav[0].cpu().numpy()\n",
    "\n",
    "# Ensure same length\n",
    "min_len = min(len(noisy_np), len(clean_np), len(denoised_np))\n",
    "noisy_np = noisy_np[:min_len]\n",
    "clean_np = clean_np[:min_len]\n",
    "denoised_np = denoised_np[:min_len]\n",
    "\n",
    "print(\"üéß Audio Comparison (play each to compare):\")\n",
    "print(\"\\n1. Noisy Input:\")\n",
    "ipd.display(ipd.Audio(noisy_np, rate=data_cfg['sample_rate']))\n",
    "\n",
    "print(\"\\n2. Denoised Output:\")\n",
    "ipd.display(ipd.Audio(denoised_np, rate=data_cfg['sample_rate']))\n",
    "\n",
    "print(\"\\n3. Clean Reference:\")\n",
    "ipd.display(ipd.Audio(clean_np, rate=data_cfg['sample_rate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize spectrograms\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for ax, (audio, title) in zip(axes, [(noisy_np, 'Noisy'), (denoised_np, 'Denoised'), (clean_np, 'Clean')]):\n",
    "    D = librosa.amplitude_to_db(np.abs(librosa.stft(audio)), ref=np.max)\n",
    "    librosa.display.specshow(D, sr=data_cfg['sample_rate'], hop_length=128, \n",
    "                            x_axis='time', y_axis='hz', ax=ax)\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylim(0, 8000)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('spectrogram_comparison.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Spectrogram comparison saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Download Model\n",
    "\n",
    "Download your trained model to use locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the best model\n",
    "from google.colab import files\n",
    "\n",
    "print(\"üì• Downloading trained model...\")\n",
    "files.download(str(ckpt_dir / 'best_model.pt'))\n",
    "print(\"\\n‚úÖ Download started! Check your browser downloads.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Copy to Google Drive for persistent storage\n",
    "SAVE_TO_GDRIVE = False  # Set to True to save to Google Drive\n",
    "GDRIVE_SAVE_PATH = \"/content/drive/MyDrive/speech_denoising_models\"\n",
    "\n",
    "if SAVE_TO_GDRIVE:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    import shutil\n",
    "    save_path = Path(GDRIVE_SAVE_PATH)\n",
    "    save_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Copy model\n",
    "    shutil.copy(ckpt_dir / 'best_model.pt', save_path / 'best_model.pt')\n",
    "    \n",
    "    # Copy training history plot\n",
    "    if Path('training_history.png').exists():\n",
    "        shutil.copy('training_history.png', save_path / 'training_history.png')\n",
    "    \n",
    "    print(f\"‚úÖ Model saved to Google Drive: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Notes\n",
    "\n",
    "- **Training time**: ~1-2 hours on Colab GPU (T4/P100) for 50 epochs\n",
    "- **Memory**: Model uses ~4-6GB GPU memory with batch size 8\n",
    "- **Best results**: Train for 100+ epochs with the full configuration\n",
    "- **Tips**:\n",
    "  - Increase batch size if you have more GPU memory\n",
    "  - Use Google Drive to persist data between sessions\n",
    "  - Check GPU usage with `!nvidia-smi`\n",
    "\n",
    "## üîó Resources\n",
    "\n",
    "- Dataset: [VoiceBank + DEMAND](https://datashare.ed.ac.uk/handle/10283/2791)\n",
    "- Paper: [A Fully Convolutional Neural Network for Speech Enhancement](https://arxiv.org/abs/1609.07132)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
