{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéôÔ∏è Speech Denoising - Google Colab Training\n",
    "\n",
    "Notebook n√†y cho ph√©p train model Speech Denoising U-Net tr√™n Google Colab v·ªõi GPU.\n",
    "\n",
    "## Overview\n",
    "- **Model**: U-Net v·ªõi Complex Ratio Mask (CRM)\n",
    "- **Dataset**: VoiceBank + DEMAND (t·ª´ Google Drive)\n",
    "- **Training Time**: ~1-2 gi·ªù tr√™n Colab GPU (T4/P100)\n",
    "\n",
    "## C·∫•u tr√∫c Dataset tr√™n Google Drive\n",
    "```\n",
    "speech_denoising_data/\n",
    "‚îú‚îÄ‚îÄ clean_trainset_28spk_wav/   (11,572 files)\n",
    "‚îú‚îÄ‚îÄ noisy_trainset_28spk_wav/   (11,572 files)\n",
    "‚îú‚îÄ‚îÄ clean_testset_wav/          (824 files)\n",
    "‚îî‚îÄ‚îÄ noisy_testset_wav/          (824 files)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected! Go to Runtime > Change runtime type > GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch torchaudio --upgrade\n",
    "!pip install -q librosa soundfile scipy numpy pandas\n",
    "!pip install -q pystoi matplotlib seaborn tensorboard\n",
    "!pip install -q tqdm pyyaml\n",
    "\n",
    "# Optional: Install PESQ\n",
    "!pip install -q pesq || echo \"PESQ installation failed - continuing without it\"\n",
    "\n",
    "print(\"\\n‚úÖ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Mount Google Drive & Setup Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üìÅ C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N GOOGLE DRIVE\n",
    "# ============================================\n",
    "\n",
    "# ƒê∆∞·ªùng d·∫´n t·ªõi folder ch·ª©a dataset tr√™n Google Drive\n",
    "# Th∆∞ m·ª•c n√†y ch·ª©a: clean_trainset_28spk_wav, noisy_trainset_28spk_wav, etc.\n",
    "GDRIVE_DATASET_FOLDER = \"speech_denoising_data\"\n",
    "\n",
    "# Folder ID t·ª´ URL (backup n·∫øu c·∫ßn)\n",
    "# URL: https://drive.google.com/drive/folders/1mDHfxtzvC-7kw0YXF0dFAcYlh7GAb2-\n",
    "GDRIVE_FOLDER_ID = \"1mDHfxtzvC-7kw0YXF0dFAcYlh7GAb2-\"\n",
    "\n",
    "print(f\"üìÇ Dataset folder: {GDRIVE_DATASET_FOLDER}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Mount drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# T·∫°o ƒë∆∞·ªùng d·∫´n ƒë·∫ßy ƒë·ªß\n",
    "GDRIVE_DATASET_PATH = f\"/content/drive/MyDrive/{GDRIVE_DATASET_FOLDER}\"\n",
    "\n",
    "# Verify dataset exists\n",
    "if os.path.exists(GDRIVE_DATASET_PATH):\n",
    "    print(f\"‚úÖ Found dataset folder: {GDRIVE_DATASET_PATH}\")\n",
    "    print(\"\\nüìÇ Contents:\")\n",
    "    for item in os.listdir(GDRIVE_DATASET_PATH):\n",
    "        item_path = os.path.join(GDRIVE_DATASET_PATH, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            count = len([f for f in os.listdir(item_path) if f.endswith('.wav')])\n",
    "            print(f\"   üìÅ {item}: {count} files\")\n",
    "else:\n",
    "    print(f\"‚ùå Dataset folder not found at: {GDRIVE_DATASET_PATH}\")\n",
    "    print(\"\\nPlease check:\")\n",
    "    print(f\"  1. Folder name is correct: {GDRIVE_DATASET_FOLDER}\")\n",
    "    print(\"  2. Dataset is in 'My Drive' root folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup dataset paths\n",
    "TRAIN_CLEAN_DIR = os.path.join(GDRIVE_DATASET_PATH, \"clean_trainset_28spk_wav\")\n",
    "TRAIN_NOISY_DIR = os.path.join(GDRIVE_DATASET_PATH, \"noisy_trainset_28spk_wav\")\n",
    "TEST_CLEAN_DIR = os.path.join(GDRIVE_DATASET_PATH, \"clean_testset_wav\")\n",
    "TEST_NOISY_DIR = os.path.join(GDRIVE_DATASET_PATH, \"noisy_testset_wav\")\n",
    "\n",
    "# Verify all directories\n",
    "print(\"üìä Dataset verification:\")\n",
    "print(\"-\" * 50)\n",
    "for name, path in [(\"Train Clean\", TRAIN_CLEAN_DIR), \n",
    "                   (\"Train Noisy\", TRAIN_NOISY_DIR),\n",
    "                   (\"Test Clean\", TEST_CLEAN_DIR),\n",
    "                   (\"Test Noisy\", TEST_NOISY_DIR)]:\n",
    "    if os.path.exists(path):\n",
    "        count = len([f for f in os.listdir(path) if f.endswith('.wav')])\n",
    "        print(f\"  ‚úÖ {name}: {count} files\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå {name}: NOT FOUND\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Define Dataset & Model Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio Processor class\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from typing import Tuple, Optional, Dict, List\n",
    "\n",
    "class AudioProcessor:\n",
    "    \"\"\"Audio processing utilities: STFT, iSTFT\"\"\"\n",
    "    \n",
    "    def __init__(self, n_fft=512, hop_length=128, win_length=512, sample_rate=16000):\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.win_length = win_length\n",
    "        self.sample_rate = sample_rate\n",
    "        self.window = torch.hann_window(win_length)\n",
    "    \n",
    "    def stft(self, waveform: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"STFT: [batch, samples] -> [batch, freq, time, 2]\"\"\"\n",
    "        if waveform.dim() == 1:\n",
    "            waveform = waveform.unsqueeze(0)\n",
    "        window = self.window.to(waveform.device)\n",
    "        stft_out = torch.stft(\n",
    "            waveform, n_fft=self.n_fft, hop_length=self.hop_length,\n",
    "            win_length=self.win_length, window=window,\n",
    "            return_complex=True, center=True, pad_mode='reflect'\n",
    "        )\n",
    "        return torch.stack([stft_out.real, stft_out.imag], dim=-1)\n",
    "    \n",
    "    def istft(self, stft_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"iSTFT: [batch, freq, time, 2] -> [batch, samples]\"\"\"\n",
    "        window = self.window.to(stft_tensor.device)\n",
    "        stft_complex = torch.complex(stft_tensor[..., 0], stft_tensor[..., 1])\n",
    "        return torch.istft(\n",
    "            stft_complex, n_fft=self.n_fft, hop_length=self.hop_length,\n",
    "            win_length=self.win_length, window=window,\n",
    "            center=True, return_complex=False\n",
    "        )\n",
    "\n",
    "print(\"‚úÖ AudioProcessor defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "class VoiceBankDEMANDDataset(Dataset):\n",
    "    \"\"\"VoiceBank + DEMAND Dataset for Speech Denoising\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        clean_dir: str,\n",
    "        noisy_dir: str,\n",
    "        sample_rate: int = 16000,\n",
    "        segment_length: int = 32000,\n",
    "        n_fft: int = 512,\n",
    "        hop_length: int = 128,\n",
    "        win_length: int = 512,\n",
    "        is_train: bool = True\n",
    "    ):\n",
    "        self.clean_dir = Path(clean_dir)\n",
    "        self.noisy_dir = Path(noisy_dir)\n",
    "        self.sample_rate = sample_rate\n",
    "        self.segment_length = segment_length\n",
    "        self.is_train = is_train\n",
    "        \n",
    "        self.audio_processor = AudioProcessor(\n",
    "            n_fft=n_fft, hop_length=hop_length,\n",
    "            win_length=win_length, sample_rate=sample_rate\n",
    "        )\n",
    "        \n",
    "        # Get file list\n",
    "        self.clean_files = sorted(list(self.clean_dir.glob(\"*.wav\")))\n",
    "        self.noisy_files = sorted(list(self.noisy_dir.glob(\"*.wav\")))\n",
    "        \n",
    "        # Match files by name\n",
    "        clean_names = {f.stem: f for f in self.clean_files}\n",
    "        noisy_names = {f.stem: f for f in self.noisy_files}\n",
    "        common_names = set(clean_names.keys()) & set(noisy_names.keys())\n",
    "        \n",
    "        self.file_pairs = [(clean_names[n], noisy_names[n]) for n in sorted(common_names)]\n",
    "        print(f\"  Found {len(self.file_pairs)} file pairs\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        clean_path, noisy_path = self.file_pairs[idx]\n",
    "        \n",
    "        # Load audio\n",
    "        clean_wav, sr = torchaudio.load(clean_path)\n",
    "        noisy_wav, _ = torchaudio.load(noisy_path)\n",
    "        \n",
    "        # Convert to mono and squeeze\n",
    "        if clean_wav.shape[0] > 1:\n",
    "            clean_wav = clean_wav.mean(dim=0)\n",
    "        else:\n",
    "            clean_wav = clean_wav.squeeze(0)\n",
    "            \n",
    "        if noisy_wav.shape[0] > 1:\n",
    "            noisy_wav = noisy_wav.mean(dim=0)\n",
    "        else:\n",
    "            noisy_wav = noisy_wav.squeeze(0)\n",
    "        \n",
    "        # Resample if needed\n",
    "        if sr != self.sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.sample_rate)\n",
    "            clean_wav = resampler(clean_wav)\n",
    "            noisy_wav = resampler(noisy_wav)\n",
    "        \n",
    "        # Random segment for training, full audio for validation\n",
    "        if self.is_train and len(clean_wav) > self.segment_length:\n",
    "            start = random.randint(0, len(clean_wav) - self.segment_length)\n",
    "            clean_wav = clean_wav[start:start + self.segment_length]\n",
    "            noisy_wav = noisy_wav[start:start + self.segment_length]\n",
    "        else:\n",
    "            # Pad or truncate\n",
    "            if len(clean_wav) < self.segment_length:\n",
    "                pad_len = self.segment_length - len(clean_wav)\n",
    "                clean_wav = torch.nn.functional.pad(clean_wav, (0, pad_len))\n",
    "                noisy_wav = torch.nn.functional.pad(noisy_wav, (0, pad_len))\n",
    "            else:\n",
    "                clean_wav = clean_wav[:self.segment_length]\n",
    "                noisy_wav = noisy_wav[:self.segment_length]\n",
    "        \n",
    "        # Compute STFT\n",
    "        clean_stft = self.audio_processor.stft(clean_wav).squeeze(0)\n",
    "        noisy_stft = self.audio_processor.stft(noisy_wav).squeeze(0)\n",
    "        \n",
    "        return {\n",
    "            'clean': clean_wav,\n",
    "            'noisy': noisy_wav,\n",
    "            'clean_stft': clean_stft,\n",
    "            'noisy_stft': noisy_stft,\n",
    "            'filename': clean_path.stem\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ VoiceBankDEMANDDataset defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Net Model\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"Conv block with BatchNorm and LeakyReLU\"\"\"\n",
    "    def __init__(self, in_ch, out_ch, kernel_size=3, stride=1, padding=1, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size, stride, padding),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(dropout) if dropout > 0 else nn.Identity()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"Encoder block with downsampling\"\"\"\n",
    "    def __init__(self, in_ch, out_ch, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.conv1 = ConvBlock(in_ch, out_ch, dropout=dropout)\n",
    "        self.conv2 = ConvBlock(out_ch, out_ch, dropout=dropout)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return self.pool(x), x\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"Decoder block with upsampling and skip connection\"\"\"\n",
    "    def __init__(self, in_ch, out_ch, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_ch, out_ch, kernel_size=2, stride=2)\n",
    "        self.conv1 = ConvBlock(out_ch * 2, out_ch, dropout=dropout)\n",
    "        self.conv2 = ConvBlock(out_ch, out_ch, dropout=dropout)\n",
    "    \n",
    "    def forward(self, x, skip):\n",
    "        x = self.up(x)\n",
    "        # Handle size mismatch\n",
    "        if x.shape != skip.shape:\n",
    "            x = F.interpolate(x, size=skip.shape[2:], mode='bilinear', align_corners=False)\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.conv2(x)\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    \"\"\"Self-attention block\"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.query = nn.Conv2d(channels, channels // 8, 1)\n",
    "        self.key = nn.Conv2d(channels, channels // 8, 1)\n",
    "        self.value = nn.Conv2d(channels, channels, 1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        q = self.query(x).view(B, -1, H * W).permute(0, 2, 1)\n",
    "        k = self.key(x).view(B, -1, H * W)\n",
    "        v = self.value(x).view(B, -1, H * W)\n",
    "        \n",
    "        attn = F.softmax(torch.bmm(q, k), dim=-1)\n",
    "        out = torch.bmm(v, attn.permute(0, 2, 1)).view(B, C, H, W)\n",
    "        return self.gamma * out + x\n",
    "\n",
    "class UNetDenoiser(nn.Module):\n",
    "    \"\"\"U-Net for speech denoising with Complex Ratio Mask\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=2,\n",
    "        out_channels=2,\n",
    "        encoder_channels=[32, 64, 128, 256, 512],\n",
    "        use_attention=True,\n",
    "        dropout=0.1,\n",
    "        mask_type='CRM'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.mask_type = mask_type\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoders = nn.ModuleList()\n",
    "        in_ch = in_channels\n",
    "        for out_ch in encoder_channels:\n",
    "            self.encoders.append(EncoderBlock(in_ch, out_ch, dropout))\n",
    "            in_ch = out_ch\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            ConvBlock(encoder_channels[-1], encoder_channels[-1] * 2, dropout=dropout),\n",
    "            AttentionBlock(encoder_channels[-1] * 2) if use_attention else nn.Identity(),\n",
    "            ConvBlock(encoder_channels[-1] * 2, encoder_channels[-1] * 2, dropout=dropout)\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoders = nn.ModuleList()\n",
    "        decoder_channels = encoder_channels[::-1]\n",
    "        in_ch = encoder_channels[-1] * 2\n",
    "        for out_ch in decoder_channels:\n",
    "            self.decoders.append(DecoderBlock(in_ch, out_ch, dropout))\n",
    "            in_ch = out_ch\n",
    "        \n",
    "        # Output\n",
    "        self.output = nn.Conv2d(encoder_channels[0], out_channels, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Store input for mask application\n",
    "        input_stft = x\n",
    "        \n",
    "        # Encoder\n",
    "        skips = []\n",
    "        for encoder in self.encoders:\n",
    "            x, skip = encoder(x)\n",
    "            skips.append(skip)\n",
    "        \n",
    "        # Bottleneck\n",
    "        x = self.bottleneck(x)\n",
    "        \n",
    "        # Decoder\n",
    "        for decoder, skip in zip(self.decoders, reversed(skips)):\n",
    "            x = decoder(x, skip)\n",
    "        \n",
    "        # Output mask\n",
    "        mask = self.output(x)\n",
    "        \n",
    "        # Apply Complex Ratio Mask\n",
    "        if self.mask_type == 'CRM':\n",
    "            mask = torch.tanh(mask)\n",
    "            output = input_stft * mask\n",
    "        else:\n",
    "            output = mask\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"‚úÖ UNetDenoiser defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "class MultiResolutionSTFTLoss(nn.Module):\n",
    "    \"\"\"Multi-resolution STFT loss\"\"\"\n",
    "    def __init__(self, fft_sizes=[512, 1024, 2048], hop_sizes=[128, 256, 512], win_lengths=[512, 1024, 2048]):\n",
    "        super().__init__()\n",
    "        self.fft_sizes = fft_sizes\n",
    "        self.hop_sizes = hop_sizes\n",
    "        self.win_lengths = win_lengths\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        loss = 0\n",
    "        for fft_size, hop_size, win_length in zip(self.fft_sizes, self.hop_sizes, self.win_lengths):\n",
    "            window = torch.hann_window(win_length).to(pred.device)\n",
    "            \n",
    "            pred_stft = torch.stft(pred, fft_size, hop_size, win_length, window, return_complex=True)\n",
    "            target_stft = torch.stft(target, fft_size, hop_size, win_length, window, return_complex=True)\n",
    "            \n",
    "            pred_mag = pred_stft.abs()\n",
    "            target_mag = target_stft.abs()\n",
    "            \n",
    "            # Spectral convergence + Log magnitude loss\n",
    "            loss += torch.norm(target_mag - pred_mag, p='fro') / (torch.norm(target_mag, p='fro') + 1e-8)\n",
    "            loss += F.l1_loss(torch.log(pred_mag + 1e-8), torch.log(target_mag + 1e-8))\n",
    "        \n",
    "        return loss / len(self.fft_sizes)\n",
    "\n",
    "class DenoiserLoss(nn.Module):\n",
    "    \"\"\"Combined loss for speech denoising\"\"\"\n",
    "    def __init__(self, complex_weight=1.0, magnitude_weight=1.0, stft_weight=0.5, n_fft=512, hop_length=128, win_length=512, use_mr_stft=True):\n",
    "        super().__init__()\n",
    "        self.complex_weight = complex_weight\n",
    "        self.magnitude_weight = magnitude_weight\n",
    "        self.stft_weight = stft_weight\n",
    "        self.use_mr_stft = use_mr_stft\n",
    "        \n",
    "        if use_mr_stft:\n",
    "            self.mr_stft_loss = MultiResolutionSTFTLoss()\n",
    "    \n",
    "    def forward(self, pred_stft, target_stft, pred_wav=None, target_wav=None):\n",
    "        losses = {}\n",
    "        \n",
    "        # Complex L1 loss\n",
    "        complex_loss = F.l1_loss(pred_stft, target_stft)\n",
    "        losses['complex_loss'] = complex_loss\n",
    "        \n",
    "        # Magnitude loss\n",
    "        pred_mag = torch.sqrt(pred_stft[:, 0]**2 + pred_stft[:, 1]**2 + 1e-8)\n",
    "        target_mag = torch.sqrt(target_stft[:, 0]**2 + target_stft[:, 1]**2 + 1e-8)\n",
    "        magnitude_loss = F.l1_loss(pred_mag, target_mag)\n",
    "        losses['magnitude_loss'] = magnitude_loss\n",
    "        \n",
    "        # MR-STFT loss\n",
    "        stft_loss = torch.tensor(0.0, device=pred_stft.device)\n",
    "        if self.use_mr_stft and pred_wav is not None and target_wav is not None:\n",
    "            stft_loss = self.mr_stft_loss(pred_wav, target_wav)\n",
    "            losses['stft_loss'] = stft_loss\n",
    "        \n",
    "        # Total loss\n",
    "        total = self.complex_weight * complex_loss + self.magnitude_weight * magnitude_loss + self.stft_weight * stft_loss\n",
    "        losses['total_loss'] = total\n",
    "        \n",
    "        return losses\n",
    "\n",
    "print(\"‚úÖ Loss functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "from pystoi import stoi\n",
    "\n",
    "def calculate_si_sdr(reference, estimation):\n",
    "    \"\"\"Calculate Scale-Invariant SDR\"\"\"\n",
    "    reference = reference - reference.mean()\n",
    "    estimation = estimation - estimation.mean()\n",
    "    \n",
    "    dot = (reference * estimation).sum()\n",
    "    s_target = dot * reference / (reference ** 2).sum()\n",
    "    e_noise = estimation - s_target\n",
    "    \n",
    "    si_sdr = 10 * torch.log10((s_target ** 2).sum() / ((e_noise ** 2).sum() + 1e-8) + 1e-8)\n",
    "    return si_sdr.item()\n",
    "\n",
    "def evaluate_batch(clean_wav, pred_wav, sample_rate=16000, compute_pesq=False, compute_stoi=True):\n",
    "    \"\"\"Evaluate batch of audio samples\"\"\"\n",
    "    metrics = {'stoi': 0.0, 'si_sdr': 0.0}\n",
    "    batch_size = clean_wav.shape[0]\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        clean = clean_wav[i].cpu().numpy()\n",
    "        pred = pred_wav[i].cpu().numpy()\n",
    "        \n",
    "        # STOI\n",
    "        if compute_stoi:\n",
    "            try:\n",
    "                metrics['stoi'] += stoi(clean, pred, sample_rate, extended=False)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # SI-SDR\n",
    "        metrics['si_sdr'] += calculate_si_sdr(\n",
    "            torch.from_numpy(clean), \n",
    "            torch.from_numpy(pred)\n",
    "        )\n",
    "    \n",
    "    # Average\n",
    "    for key in metrics:\n",
    "        metrics[key] /= batch_size\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"‚úÖ Metrics defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Configuration & Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration (optimized for Colab GPU)\n",
    "CONFIG = {\n",
    "    'data': {\n",
    "        'sample_rate': 16000,\n",
    "        'segment_length': 32000,  # 2 seconds\n",
    "    },\n",
    "    'stft': {\n",
    "        'n_fft': 512,\n",
    "        'hop_length': 128,\n",
    "        'win_length': 512,\n",
    "    },\n",
    "    'model': {\n",
    "        'encoder_channels': [32, 64, 128, 256, 512],\n",
    "        'use_attention': True,\n",
    "        'dropout': 0.1,\n",
    "    },\n",
    "    'training': {\n",
    "        'batch_size': 8,        # Ph√π h·ª£p v·ªõi GPU memory\n",
    "        'num_epochs': 50,       # TƒÉng l√™n 100 n·∫øu c√≥ th·ªùi gian\n",
    "        'learning_rate': 0.0001,\n",
    "        'weight_decay': 1e-5,\n",
    "        'grad_clip': 5.0,\n",
    "        'num_workers': 2,\n",
    "        'use_amp': True,        # Mixed precision\n",
    "        'early_stopping_patience': 10,\n",
    "    },\n",
    "    'scheduler': {\n",
    "        'patience': 5,\n",
    "        'factor': 0.5,\n",
    "        'min_lr': 1e-6,\n",
    "    },\n",
    "    'loss': {\n",
    "        'complex_weight': 1.0,\n",
    "        'magnitude_weight': 1.0,\n",
    "        'stft_weight': 0.5,\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"üìã Configuration:\")\n",
    "print(f\"  Batch size: {CONFIG['training']['batch_size']}\")\n",
    "print(f\"  Epochs: {CONFIG['training']['num_epochs']}\")\n",
    "print(f\"  Learning rate: {CONFIG['training']['learning_rate']}\")\n",
    "print(f\"  Mixed precision: {CONFIG['training']['use_amp']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets and dataloaders\n",
    "print(\"üìÇ Loading dataset from Google Drive...\")\n",
    "\n",
    "stft_cfg = CONFIG['stft']\n",
    "data_cfg = CONFIG['data']\n",
    "train_cfg = CONFIG['training']\n",
    "\n",
    "# Training dataset\n",
    "print(\"\\n  Loading training set...\")\n",
    "train_dataset = VoiceBankDEMANDDataset(\n",
    "    clean_dir=TRAIN_CLEAN_DIR,\n",
    "    noisy_dir=TRAIN_NOISY_DIR,\n",
    "    sample_rate=data_cfg['sample_rate'],\n",
    "    segment_length=data_cfg['segment_length'],\n",
    "    n_fft=stft_cfg['n_fft'],\n",
    "    hop_length=stft_cfg['hop_length'],\n",
    "    win_length=stft_cfg['win_length'],\n",
    "    is_train=True\n",
    ")\n",
    "\n",
    "# Validation dataset\n",
    "print(\"  Loading validation set...\")\n",
    "val_dataset = VoiceBankDEMANDDataset(\n",
    "    clean_dir=TEST_CLEAN_DIR,\n",
    "    noisy_dir=TEST_NOISY_DIR,\n",
    "    sample_rate=data_cfg['sample_rate'],\n",
    "    segment_length=data_cfg['segment_length'],\n",
    "    n_fft=stft_cfg['n_fft'],\n",
    "    hop_length=stft_cfg['hop_length'],\n",
    "    win_length=stft_cfg['win_length'],\n",
    "    is_train=False\n",
    ")\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=train_cfg['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=train_cfg['num_workers'],\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=train_cfg['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=train_cfg['num_workers'],\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Data loaded!\")\n",
    "print(f\"   Training samples: {len(train_dataset)}\")\n",
    "print(f\"   Validation samples: {len(val_dataset)}\")\n",
    "print(f\"   Training batches: {len(train_loader)}\")\n",
    "print(f\"   Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_cfg = CONFIG['model']\n",
    "\n",
    "model = UNetDenoiser(\n",
    "    in_channels=2,\n",
    "    out_channels=2,\n",
    "    encoder_channels=model_cfg['encoder_channels'],\n",
    "    use_attention=model_cfg['use_attention'],\n",
    "    dropout=model_cfg['dropout'],\n",
    "    mask_type='CRM'\n",
    ").to(device)\n",
    "\n",
    "print(f\"üß† Model: UNetDenoiser\")\n",
    "print(f\"   Parameters: {model.count_parameters():,}\")\n",
    "print(f\"   Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize training components\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Directories\n",
    "ckpt_dir = Path('./checkpoints')\n",
    "ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Loss function\n",
    "loss_cfg = CONFIG['loss']\n",
    "criterion = DenoiserLoss(\n",
    "    complex_weight=loss_cfg['complex_weight'],\n",
    "    magnitude_weight=loss_cfg['magnitude_weight'],\n",
    "    stft_weight=loss_cfg['stft_weight'],\n",
    "    use_mr_stft=True\n",
    ").to(device)\n",
    "\n",
    "# Audio processor for iSTFT\n",
    "audio_processor = AudioProcessor(\n",
    "    n_fft=stft_cfg['n_fft'],\n",
    "    hop_length=stft_cfg['hop_length'],\n",
    "    win_length=stft_cfg['win_length']\n",
    ")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=train_cfg['learning_rate'],\n",
    "    weight_decay=train_cfg['weight_decay']\n",
    ")\n",
    "\n",
    "# Scheduler\n",
    "scheduler_cfg = CONFIG['scheduler']\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=scheduler_cfg['factor'],\n",
    "    patience=scheduler_cfg['patience'],\n",
    "    min_lr=scheduler_cfg['min_lr']\n",
    ")\n",
    "\n",
    "# Mixed precision scaler\n",
    "scaler = GradScaler() if train_cfg['use_amp'] else None\n",
    "\n",
    "print(\"‚úÖ Training components initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training functions\n",
    "def train_epoch(model, train_loader, optimizer, criterion, audio_processor, device, scaler=None):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=\"Training\")\n",
    "    for batch in pbar:\n",
    "        noisy_stft = batch['noisy_stft'].to(device)\n",
    "        clean_stft = batch['clean_stft'].to(device)\n",
    "        clean_wav = batch['clean'].to(device)\n",
    "        \n",
    "        # Reshape: [batch, freq, time, 2] -> [batch, 2, freq, time]\n",
    "        noisy_stft = noisy_stft.permute(0, 3, 1, 2)\n",
    "        clean_stft = clean_stft.permute(0, 3, 1, 2)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if scaler is not None:\n",
    "            with autocast():\n",
    "                pred_stft = model(noisy_stft)\n",
    "                \n",
    "                # Reconstruct waveform\n",
    "                pred_stft_istft = pred_stft.permute(0, 2, 3, 1)\n",
    "                pred_wav = audio_processor.istft(pred_stft_istft)\n",
    "                \n",
    "                # Ensure same length\n",
    "                min_len = min(pred_wav.shape[-1], clean_wav.shape[-1])\n",
    "                pred_wav = pred_wav[..., :min_len]\n",
    "                clean_wav_trim = clean_wav[..., :min_len]\n",
    "                \n",
    "                losses = criterion(pred_stft, clean_stft, pred_wav, clean_wav_trim)\n",
    "            \n",
    "            scaler.scale(losses['total_loss']).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), train_cfg['grad_clip'])\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            pred_stft = model(noisy_stft)\n",
    "            pred_stft_istft = pred_stft.permute(0, 2, 3, 1)\n",
    "            pred_wav = audio_processor.istft(pred_stft_istft)\n",
    "            \n",
    "            min_len = min(pred_wav.shape[-1], clean_wav.shape[-1])\n",
    "            pred_wav = pred_wav[..., :min_len]\n",
    "            clean_wav_trim = clean_wav[..., :min_len]\n",
    "            \n",
    "            losses = criterion(pred_stft, clean_stft, pred_wav, clean_wav_trim)\n",
    "            losses['total_loss'].backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), train_cfg['grad_clip'])\n",
    "            optimizer.step()\n",
    "        \n",
    "        total_loss += losses['total_loss'].item()\n",
    "        num_batches += 1\n",
    "        pbar.set_postfix({'loss': f\"{losses['total_loss'].item():.4f}\"})\n",
    "    \n",
    "    return total_loss / num_batches\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, val_loader, criterion, audio_processor, device):\n",
    "    \"\"\"Validate model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    metrics = {'stoi': 0, 'si_sdr': 0}\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "        noisy_stft = batch['noisy_stft'].to(device)\n",
    "        clean_stft = batch['clean_stft'].to(device)\n",
    "        clean_wav = batch['clean'].to(device)\n",
    "        \n",
    "        noisy_stft = noisy_stft.permute(0, 3, 1, 2)\n",
    "        clean_stft = clean_stft.permute(0, 3, 1, 2)\n",
    "        \n",
    "        pred_stft = model(noisy_stft)\n",
    "        pred_stft_istft = pred_stft.permute(0, 2, 3, 1)\n",
    "        pred_wav = audio_processor.istft(pred_stft_istft)\n",
    "        \n",
    "        min_len = min(pred_wav.shape[-1], clean_wav.shape[-1])\n",
    "        pred_wav = pred_wav[..., :min_len]\n",
    "        clean_wav_trim = clean_wav[..., :min_len]\n",
    "        \n",
    "        losses = criterion(pred_stft, clean_stft)\n",
    "        total_loss += losses['total_loss'].item()\n",
    "        \n",
    "        # Metrics\n",
    "        try:\n",
    "            batch_metrics = evaluate_batch(\n",
    "                clean_wav_trim, pred_wav,\n",
    "                sample_rate=CONFIG['data']['sample_rate'],\n",
    "                compute_stoi=True\n",
    "            )\n",
    "            for key in metrics:\n",
    "                if key in batch_metrics:\n",
    "                    metrics[key] += batch_metrics[key]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    avg_metrics = {k: v / num_batches for k, v in metrics.items()}\n",
    "    \n",
    "    return avg_loss, avg_metrics\n",
    "\n",
    "print(\"‚úÖ Training functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop\n",
    "print(\"=\"*60)\n",
    "print(\"üöÄ STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Dataset: Google Drive - {GDRIVE_DATASET_FOLDER}\")\n",
    "print(f\"Epochs: {train_cfg['num_epochs']}\")\n",
    "print(f\"Batch size: {train_cfg['batch_size']}\")\n",
    "print(f\"Device: {device}\")\n",
    "print()\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "history = {'train_loss': [], 'val_loss': [], 'stoi': [], 'si_sdr': []}\n",
    "\n",
    "for epoch in range(train_cfg['num_epochs']):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{train_cfg['num_epochs']}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion, audio_processor, device, scaler)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_metrics = validate(model, val_loader, criterion, audio_processor, device)\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['stoi'].append(val_metrics.get('stoi', 0))\n",
    "    history['si_sdr'].append(val_metrics.get('si_sdr', 0))\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"  STOI: {val_metrics.get('stoi', 0):.3f}\")\n",
    "    print(f\"  SI-SDR: {val_metrics.get('si_sdr', 0):.2f} dB\")\n",
    "    print(f\"  LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "    \n",
    "    # Check for best model\n",
    "    is_best = val_loss < best_val_loss\n",
    "    if is_best:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        # Save best model\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'config': CONFIG\n",
    "        }, ckpt_dir / 'best_model.pt')\n",
    "        print(\"  ‚úÖ Saved best model!\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    # Save periodic checkpoint\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "        }, ckpt_dir / f'checkpoint_epoch_{epoch+1}.pt')\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= train_cfg['early_stopping_patience']:\n",
    "        print(f\"\\n‚èπÔ∏è Early stopping at epoch {epoch + 1}\")\n",
    "        break\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ TRAINING COMPLETED!\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"Model saved to: {ckpt_dir / 'best_model.pt'}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Loss\n",
    "axes[0, 0].plot(history['train_loss'], label='Train')\n",
    "axes[0, 0].plot(history['val_loss'], label='Validation')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Training & Validation Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# STOI\n",
    "axes[0, 1].plot(history['stoi'])\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('STOI')\n",
    "axes[0, 1].set_title('STOI (Speech Intelligibility)')\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# SI-SDR\n",
    "axes[1, 0].plot(history['si_sdr'])\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('SI-SDR (dB)')\n",
    "axes[1, 0].set_title('SI-SDR (Signal Quality)')\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Summary\n",
    "axes[1, 1].axis('off')\n",
    "axes[1, 1].text(0.5, 0.5, f'Best Val Loss: {best_val_loss:.4f}\\n\\n'\n",
    "                f'Final STOI: {history[\"stoi\"][-1]:.3f}\\n'\n",
    "                f'Final SI-SDR: {history[\"si_sdr\"][-1]:.2f} dB',\n",
    "                ha='center', va='center', fontsize=14,\n",
    "                transform=axes[1, 1].transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Training history saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load(ckpt_dir / 'best_model.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"‚úÖ Loaded best model from epoch {checkpoint['epoch'] + 1}\")\n",
    "print(f\"   Validation loss: {checkpoint['val_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a sample\n",
    "import IPython.display as ipd\n",
    "\n",
    "# Get a test sample\n",
    "test_batch = next(iter(val_loader))\n",
    "noisy_wav = test_batch['noisy'][0:1].to(device)\n",
    "clean_wav = test_batch['clean'][0:1]\n",
    "noisy_stft = test_batch['noisy_stft'][0:1].to(device)\n",
    "\n",
    "# Denoise\n",
    "with torch.no_grad():\n",
    "    noisy_stft_input = noisy_stft.permute(0, 3, 1, 2)\n",
    "    pred_stft = model(noisy_stft_input)\n",
    "    pred_stft_out = pred_stft.permute(0, 2, 3, 1)\n",
    "    denoised_wav = audio_processor.istft(pred_stft_out)\n",
    "\n",
    "# Convert to numpy\n",
    "noisy_np = noisy_wav[0].cpu().numpy()\n",
    "clean_np = clean_wav[0].numpy()\n",
    "denoised_np = denoised_wav[0].cpu().numpy()\n",
    "\n",
    "# Ensure same length\n",
    "min_len = min(len(noisy_np), len(clean_np), len(denoised_np))\n",
    "noisy_np = noisy_np[:min_len]\n",
    "clean_np = clean_np[:min_len]\n",
    "denoised_np = denoised_np[:min_len]\n",
    "\n",
    "print(\"üéß Audio Comparison:\")\n",
    "print(\"\\n1. Noisy Input:\")\n",
    "ipd.display(ipd.Audio(noisy_np, rate=CONFIG['data']['sample_rate']))\n",
    "\n",
    "print(\"\\n2. Denoised Output:\")\n",
    "ipd.display(ipd.Audio(denoised_np, rate=CONFIG['data']['sample_rate']))\n",
    "\n",
    "print(\"\\n3. Clean Reference:\")\n",
    "ipd.display(ipd.Audio(clean_np, rate=CONFIG['data']['sample_rate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize spectrograms\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for ax, (audio, title) in zip(axes, [(noisy_np, 'Noisy'), (denoised_np, 'Denoised'), (clean_np, 'Clean')]):\n",
    "    D = librosa.amplitude_to_db(np.abs(librosa.stft(audio)), ref=np.max)\n",
    "    librosa.display.specshow(D, sr=CONFIG['data']['sample_rate'], hop_length=128, \n",
    "                            x_axis='time', y_axis='hz', ax=ax)\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylim(0, 8000)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('spectrogram_comparison.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Spectrogram comparison saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Save Model to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to Google Drive\n",
    "import shutil\n",
    "\n",
    "# T·∫°o th∆∞ m·ª•c l∆∞u model tr√™n Google Drive\n",
    "GDRIVE_MODEL_SAVE_PATH = \"/content/drive/MyDrive/speech_denoising_models\"\n",
    "save_path = Path(GDRIVE_MODEL_SAVE_PATH)\n",
    "save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Copy best model\n",
    "shutil.copy(ckpt_dir / 'best_model.pt', save_path / 'best_model.pt')\n",
    "\n",
    "# Copy training history\n",
    "if Path('training_history.png').exists():\n",
    "    shutil.copy('training_history.png', save_path / 'training_history.png')\n",
    "if Path('spectrogram_comparison.png').exists():\n",
    "    shutil.copy('spectrogram_comparison.png', save_path / 'spectrogram_comparison.png')\n",
    "\n",
    "print(f\"‚úÖ Model saved to Google Drive: {save_path}\")\n",
    "print(\"   Files saved:\")\n",
    "for f in save_path.iterdir():\n",
    "    print(f\"   - {f.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Download to local machine\n",
    "from google.colab import files\n",
    "\n",
    "print(\"üì• Downloading trained model...\")\n",
    "files.download(str(ckpt_dir / 'best_model.pt'))\n",
    "print(\"\\n‚úÖ Download started! Check your browser downloads.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Notes\n",
    "\n",
    "### H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng\n",
    "1. Upload dataset l√™n Google Drive v·ªõi c·∫•u tr√∫c th∆∞ m·ª•c ƒë√∫ng\n",
    "2. S·ª≠a `GDRIVE_DATASET_FOLDER` n·∫øu t√™n th∆∞ m·ª•c kh√°c\n",
    "3. Ch·∫°y t·ª´ng cell t·ª´ ƒë·∫ßu ƒë·∫øn cu·ªëi\n",
    "4. Model s·∫Ω ƒë∆∞·ª£c l∆∞u v√†o Google Drive sau khi train xong\n",
    "\n",
    "### Training Tips\n",
    "- **Th·ªùi gian**: ~1-2 gi·ªù tr√™n Colab GPU (T4) cho 50 epochs\n",
    "- **Memory**: Model s·ª≠ d·ª•ng ~4-6GB GPU memory v·ªõi batch size 8\n",
    "- **TƒÉng epochs**: ƒê·ªïi `num_epochs` th√†nh 100 ƒë·ªÉ c√≥ k·∫øt qu·∫£ t·ªët h∆°n\n",
    "- Check GPU: `!nvidia-smi`\n",
    "\n",
    "### Sau khi train\n",
    "- Model ƒë∆∞·ª£c l∆∞u t·∫°i `./checkpoints/best_model.pt`\n",
    "- Section 7 s·∫Ω copy model l√™n Google Drive ƒë·ªÉ l∆∞u tr·ªØ\n",
    "- C√≥ th·ªÉ download model v·ªÅ m√°y local"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
